{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
    "\n",
    "#### Group Members' Names: Hina Shafique Awan, Sidda De Silva\n",
    "\n",
    "\n",
    "#### Group Members' Emails: hina.awan@ryerson.ca , ravindra.desilva@ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction:\n",
    "\n",
    "## 1.1 Problem Description:\n",
    "\n",
    "The emergence of pre-trained models has revolutionized the applications of natural language processing. However, most of the pre-trained models do not perform well across different tasks limiting their applicability. The key objective of this study is to come up with a Neural Network architecture and pre-training methods for a language model with wider applicability.\n",
    "\n",
    "## 1.2 Context of the Problem:\n",
    "\n",
    "Neural networks for language modeling have been proven effective and accurate over statistical language modeling. However, training deep language models is time-consuming and computationally intensive. Pre-trained language models are appealing and have become an industry standard given the state-of-the-art performance, and also the fact that pre-trained models are available for general immediate use. Practitioners can focus on the actual Natural Language Processing (NLP) task than spending the time, hardware, and data to train models.\n",
    "\n",
    "The benefit of pre-trained language models is that they can function as a black box that understands the language and can then be utilized to perform any specific task in that language. A Pre-trained language model is the machine equivalent of a human being who deeply understands the language. Pre-trained models can be easily incorporated into new applications as they don't need much-labeled data, making them adaptable to various business problems, prediction, transfer learning, or feature extraction. Like the human counterpart, the pre-training model should be trained on a large corpus. It is not possible to label such a large dataset, therefore self-supervised training is required.\n",
    "\n",
    "The Masked Language model is the most remarkable approach in self-supervised NLP training. However, many of the masking methods (noising schemes) are task-specific and do not generate a generic language model. BART is a Neural Network model which combines both Bidirectional and Auto-Regressive Transformers and pre trains a model using a combination of noising schemes. It can be seen as generalizing many pretraining schemes and achieving noising flexibility.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1.3 Limitation About other Approaches:\n",
    "\n",
    "### 1.3.1 BERT: \n",
    "Random tokens are replaced with masks, and the document is bidirectionally encoded. BERT does not perform very well on downstream text generation tasks as the masked tokens are predicted independently of each other.\n",
    "\n",
    "### 1.3.2 GPT: \n",
    "Tokens are predicted auto-regressively.  This means that each new prediction uses previously predicted tokens as context. This helps it perform very well on downstream text generation tasks. However, words can only condition in a leftward context, so they cannot learn bidirectional interactions.\n",
    "\n",
    "There are many masked training approaches, and some of these methods typically focus on particular types of end tasks (e.g., span prediction, generation, etc.), therefore not useful to train a generic language model.\n",
    "\n",
    "## 1.4 Solution:\n",
    "\n",
    "To pre-train a model, BART uses a combination of bidirectional and auto-regressive transformers. This technique has several advantages, including the ability to apply arbitrary alterations to the original text, such as adjusting its length. The research compares a variety of noise reduction techniques, determining the best results by shuffling the sequence of the original phrases at random and employing a new in-filling strategy in which any length text spans (including zero-length) are replaced with a single mask token.  By requiring the model to reason more about overall sentence length and apply longer-range modifications to the input, this technique generalizes BERT’s original word masking and next sentence prediction aims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background\n",
    "\n",
    "The related work is shown in the following table:\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Radford et al. [7] | GPT(Generative Pre-Training) proposes generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task like textual entailment, question answering, semantic similarity assessment, and document classification.| For natural language inference task: SNLI, MultiNLI, Question NLI, RTE, SciTail, For Question Answering task: RACE, Story Cloze, For sentence similarity task: MSR Paraphrase Corpus, Quora Question Pairs, STS Benchmark, For Classification task: Stanford Sentiment Treebank-2, CoL. An improvement of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI) has been acheived. | only models leftward context, which is problematic for some tasks.\n",
    "| Devlin et al. [5] | BERT introduced masked language modelling, which allows pre-training to learn interactions between left and right context words. Predictions are not made auto-regressively which reduces the effectiveness of BERT for generation tasks.| GLUE, MultiNLI, SQUAD v1.1, SQUAD v2.0. GLUE score is 80.5%, MultiNLI accuracy is 86.7%, F1 score of SQUAD v1.1 is 93.2, F1 score of SQUAD v2.0 is 83.1| Predictions are not made auto-regressively, reducing the effectiveness of BERT for generation tasks.\n",
    "| Dong et al. [6] | UniLM is pretrained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction for downstream tasks like extractive question answering, long text generation and abstractive summatization, repectively.| CNN/DailyMail and Gigaword abstractive summarization, SQuAD question generation, CoQA generative question answering, and DSTC7 dialog response generation. Experimental results demonstrate that the model compares favorably with BERT on the GLUE benchmark and two question answering datasets. |  UniLM predictions are conditionally independent.\n",
    "| Lewis et al. [4] | BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable in several ways for the target applications.| SQUAD for question answering, MNLI for classification, XSum for news summarization, ConvAI2 for dialogue response, ELI5 dataset for abstrative QA, CNN/Daily Mail data set for summarization | The strength of BART is also its weakness. The pretraining experimentation was to generate a generic language model that could be used across multiple downstream fine-tuning end tasks. Future work should then explore new methods of corrupting documents, maybe with the intent of being specific for certain downstream fine-tuning end tasks than being generic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Methodology\n",
    "\n",
    "## 3.1 Neural Network Architecture\n",
    "To understand the architecture of BART, it is important to comprehend the nature of the NLP tasks it aims to deal with. In-text summarization and question answering tasks, it is essential for the model to read the entire text and interpret each token in the context of what came before and after it. For example, to train a masked language model with the sentence “I am following a lecture in Natural Language Processing at a university”, the following masked sentence can be used.\n",
    "\n",
    "“I am following an [MASK] in Natural Language Processing at a university”.\n",
    "\n",
    "For a Natural Language Understanding (NLU) problem like this, it is important to completely read the sentence before predicting the [Mask] as it is highly dependent on the words \"university\" and \"following\". The bi-directional approaches to reading and representing a text can properly interpret input sequences in these cases. BERT (Devlin et al., 2019) introduced masked language\n",
    "modeling, allowing pre-training to learn interactions between left and right context words, hence improving the task of language modeling.\n",
    "\n",
    "BART uses the bi-directional encoder of BERT to find the best representation of its input sequence. The BERT encoder produces an embedding vector of each token of its input text sequence and an extra vector of sentence-level information. This helps the decoder learn token and sentence level tasks which help in fine-tuning future tasks.\n",
    "\n",
    "The masked sequences are used in the pre-training process as shown in Figure 1. While a simple token masking technique is used in pre-training the BERT model, BART utilizes a more challenging masking procedure in its pre-training.\n",
    "\n",
    "Figure 1: Bidirectional Encoder\n",
    "\n",
    "\n",
    "![Encoder](images/encoder.png)\n",
    "\n",
    "A decoder is needed, after getting the representations of an input text sequence, to map these with the target output. However, if the decoder is designed in a similar way, it can perform poorly on next sentence prediction and token prediction tasks as it depends on more diverse input.\n",
    "\n",
    "In such cases, a model architecture is needed that can be trained on producing the next word by only examining preceding words in the sequence. Hence, an autoregressive model is useful as it only looks at the past data to predict the future. \n",
    "\n",
    "The Figure 2 shows how the autoregressive decoder processes its input.\n",
    "\n",
    "Figure 2: Auto-regressive Decoder\n",
    "\n",
    "\n",
    "![Decoder](images/auto-decoder.png)\n",
    "\n",
    "BART attaches the bi-directional encoder to the autoregressive decoder to create a denoising auto-encoder architecture. Therefore, the architecture is that of a typical sequence-to-sequence transformer as seen below. Both sections of the Transformer architecture are used in sequence-to-sequence models. There would be many layers of encoders and decoders stacked. The base BART Model has 6 encoders and decoders, and the large Model has 12 layers each.  The final BART model would look something like in Figure 3:\n",
    "\n",
    "Figure 3: BART Model\n",
    "\n",
    "\n",
    "![transformer](images/transformer.png)\n",
    "\n",
    "The encoder's attention layers can access all the words in the original phrase at each step, but the decoder's attention layers can only access the words positioned before a particular word in the input. ReLU activation functions are used in traditional Sequence-to-Sequence models, although BART employs GeLUs. In the most recent Transformers, the GeLU activation function was employed in Google's BERT and OpenAI's GPT-2.\n",
    " \n",
    "\\begin{align}\n",
    "GELU(x) = \\text0.5x\\begin{pmatrix}\\text1+\\tanh\\begin{pmatrix}\\sqrt{\\frac{2}{\\pi}}(x+0.044715x^3)\\end{pmatrix}\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The Gelu function is as shown in Figure 4.\n",
    "\n",
    "Figure 4: GeLU Function\n",
    "\n",
    "![gelu](images/gelu.png)\n",
    "\n",
    "### 3.1.1 Learning\n",
    "\n",
    "Figure 5: Bidirectional Encoder Auto-regressive Decoder\n",
    "\n",
    "\n",
    "![encoder-decoder](images/encoder-decoder.png)\n",
    "\n",
    "In the above Figure 5, the input sequence is a masked (or noisy) version of [ABCDE] transformed into [A[MASK]B[MASK]E]. The encoder examines the whole sequence and learns high-dimensional representations with bi-directional information. The autoregressive decoder takes these thought vectors and predicts the next token based on the encoder input and the output tokens predicted so far. Learning occurs by computing and optimizing the negative log-likelihood as mapped with the target [ABCDE]. \n",
    "\n",
    "The following Figure 6 is a simplified view of the learning process.\n",
    "\n",
    "Figure 6: BART Training Process\n",
    "\n",
    "\n",
    "![bart-training](images/bart-training.png)\n",
    "\n",
    "## 3.2 Pretraining objectives/tasks\n",
    "BART architecture is quite similar to a typical sequence-to-sequence transformer. The main improvement through BART is an exploration of the numerous pretraining activities. Many other models picked a training objective and measured the performance improvement. However, in BART training many pre-training tasks were carried over numerous datasets to identify the effectiveness.\n",
    "\n",
    "The goal of the pretraining exercises is to recover from document corruption. There are five different sorts of \"noising\" methods employed as can be seen in Figure 7. The model will learn to generalize by using multiple tasks.\n",
    "\n",
    "Figure 7: Pretraining Tasks\n",
    "\n",
    "\n",
    "![masking](images/masking.png)\n",
    "\n",
    "### 3.2.1 Token masking\n",
    "Follows BERT where some tokens are masked, and the model needs to predict the masked token.\n",
    "\n",
    "### 3.2.2 Token deletion\n",
    "Delete token and make the model to restore deleted token at the right position.\n",
    "\n",
    "### 3.2.3 Text infilling\n",
    "Multiple words are selected in a span, and replaced with single MASK token. This will teach model to predict how many tokens are missing.\n",
    "\n",
    "### 3.2.4 Sentence permutation\n",
    "Shuffle sentences and make model restore them.\n",
    "\n",
    "### 3.2.5 Document rotation\n",
    "Select random token. Change document ordering to start from selected token. Make model predict the start of the original document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original implementation of the paper is in the [initial commit at fairseq github repository](https://github.com/pytorch/fairseq/commit/a92bcdad5a0dea6a440cc92976e4166811b16671). Thereafter there have been many improvements to the model at [fairseq github repository](https://github.com/pytorch/fairseq/tree/main/fairseq/models/bart). However, the [Huggingface implementation](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bart) is the more standard and reference in the project. Huggingface contains transformer library code which BART implementation was able to base on.\n",
    "\n",
    "Unlike BERT, which only has an encoder, and GPT, which only has a decoder, BART has both an encoder and a decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class BartModel(BartPretrainedModel):\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
    "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
    "\n",
    "        self.encoder = BartEncoder(config, self.shared)\n",
    "        self.decoder = BartDecoder(config, self.shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder has multiple encoder layers and a positional embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      self.embed_positions = BartLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            embed_dim,\n",
    "        )\n",
    "        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder also has multiple decoder layers and a positional embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            config.d_model,\n",
    "        )\n",
    "        self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(config.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each encoder and decoder layer has self attention which is Multi-headed attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.self_attn = BartAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.decoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of tokenization BART uses byte-level Byte-Pair-Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Fine tuning\n",
    "\n",
    "Pretraining is the process of training a model from the ground up: the weights are set at random, and the training begins with no past information. This sort of model creates a statistical comprehension of the language it was trained on, but it isn't very effective for specific tasks. On the other hand, fine-tuning refers to the training that occurs after a model has been pre-trained. As a result, the general pre-trained model undergoes a process known as transfer learning. The model is fine-tuned in a supervised manner — that is, using human-annotated labels — on a specific task during this process. The question arises, why not just train for the final task straight away? There are several factors at play: \n",
    "\n",
    "- The pretrained model had already been trained on a dataset that resembled the fine-tuning dataset in certain ways. As a result, the fine-tuning procedure might benefit from the knowledge gained by the original model during pretraining.  In NLP tasks, the pretrained model will have some statistical grasp of the related language.\n",
    "- Because the pre-trained model has already been trained on a large amount of data, fine-tuning takes far less data to achieve acceptable results. As a result, the time and resources required to get good outcomes are significantly reduced.  For example, a science/research-based model may be created by using a pre-trained model trained on the English language and then fine-tuning it on an arXiv corpus. \n",
    "- The fine-tuning will only need a little quantity of data: the information obtained by the pre-trained model is \"transferred,\" which is why it is named transfer learning. \n",
    "\n",
    "The following are a few NLP final tasks using the pre-trained BART model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Text summarization\n",
    "\n",
    "Text summarization reduces the number of sentences and words in a document while preserving its meaning. Instead of training on a large corpus, the BART pre-trained model is used. BART pre-trained model already captured the patterns in the English language. Then BART pre-trained model is fine-tuned on CNN Daily Mail data. Therefore, the final fine-tuned model has captured the specifics of the CNN Daily Mail on top of the general BART language model.\n",
    "The input sequence is fed to the encoder, while the decoder autoregressively generates output as can be seen in Figure 8.\n",
    "\n",
    "Figure 8: Text summarization\n",
    "\n",
    "![bart-input-output](images/bart-input-output.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Microsoft Windows. The company's 1986 initial public offering (IPO), and subsequent rise in its share price, created three billionaires and an estimated 12,000 millionaires among Microsoft employees. In 2018, Microsoft reclaimed its position as the most valuable publicly traded company in the world. As of 2020, Microsoft has the third-highest global brand valuation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "import wikipedia\n",
    "wikisearch = wikipedia.page(\"Microsoft\")\n",
    "wikicontent = wikisearch.content\n",
    "\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus([wikicontent],return_tensors='pt',truncation=True)\n",
    "summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
    "\n",
    "bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(bart_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Question and answering\n",
    "In the context of NLP, question and answering attempts to answer questions from humans. Since the questions are from humans the questions are in natural language, and the answer has to also be in natural language. The general English language BART is further trained on the SQuADv2 dataset.\n",
    "The entire document is fed into the encoder and decoder, and the top hidden state of the decoder is used as a representation for each word. This representation is used to identify the answer tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as Amazonia or the Amazon Jungle\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('a-ware/bart-squadv2')\n",
    "model = BartForQuestionAnswering.from_pretrained('a-ware/bart-squadv2')\n",
    "\n",
    "question, text = \"Which name is also used to describe the Amazon rainforest in English?\", \"The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain 'Amazonas' in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\"\n",
    "encoding = tokenizer(question, text, return_tensors='pt')\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "start_scores, end_scores = model(input_ids, attention_mask=attention_mask, output_attentions=False)[:2]\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "answer = tokenizer.convert_tokens_to_ids(answer.split())\n",
    "answer = tokenizer.decode(answer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning can be done in different ways. The following is another example of fine tuning the generic English language for the same question and answering scenario above. The same SQuADv2 dataset is used to generate another fine tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Primer/bart-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"Primer/bart-squad2\")\n",
    "# model.to('cuda'); \n",
    "model.eval()\n",
    "\n",
    "def answer(question, text):\n",
    "    seq = '<s>' +  question + ' </s> </s> ' + text + ' </s>'\n",
    "    tokens = tokenizer.encode_plus(seq, return_tensors='pt', padding='max_length', max_length=1024)\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    start, end= model(input_ids, attention_mask=attention_mask)[:2]\n",
    "    start_idx = int(start.argmax().int())\n",
    "    end_idx =  int(end.argmax().int())\n",
    "    print(tokenizer.decode(input_ids[0, start_idx:end_idx]).strip())\n",
    "    \n",
    "\n",
    "question = \"Where does Tom live?\"\n",
    "context = \"Tom is an engineer in San Francisco.\"\n",
    "answer(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazonia or the Amazon Jungle\n"
     ]
    }
   ],
   "source": [
    "question, text = \"Which name is also used to describe the Amazon rainforest in English?\", \"The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain 'Amazonas' in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\"\n",
    "answer(question, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco\n"
     ]
    }
   ],
   "source": [
    "question = \"Where does Tom live?\"\n",
    "context = \"Tom is an engineer in San Francisco.\"\n",
    "answer(question, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Classification\n",
    "In text classification, the text is analyzed and then assigned predefined categories. \n",
    "\n",
    "#### 4.1.2.1 Zero-shot Classification\n",
    "The generic BART English language model was further trained using Yahoo Answers topic classification. Zero-shot models are used to classify data, which wasn’t used to train the model.\n",
    "The same input sequence is fed to the encoder and decoder.\n",
    "The final hidden state of the final decoder token is used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.994511067867279, 0.005706192459911108, 0.0018192973220720887]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels, multi_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.2 Classification on a fine tuned model of a custom supervised dataset\n",
    "\n",
    "In the earlier section classification was performed using a generic trained model. In this section, BART and BERT base models will be trained (fine-tuned) on a customer supervised (labelled) dataset.\n",
    "The custom dataset used in this experiment is a dataset downloadable from https://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data and pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/bart-clf-consumer-complaint.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data row has 4 fields\n",
    " - Id of the complaint\n",
    " - The product category description\n",
    " - The complaint narrative\n",
    " - The product category label\n",
    "\n",
    "The purpose of the classification is to identify the potential product category labels given a complaint narrative.\n",
    "Multiple product category labels are possible. However, for model comparison purposes the most likely product category is chosen.\n",
    "There are 5 potential product category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit-reporting    9186\n",
       "debt-collection     2285\n",
       "mortgages-loans     1831\n",
       "credit-card         1557\n",
       "retail-banking      1383\n",
       "Name: product, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "data['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only 100 samples to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby('product').apply(lambda x: x.sample(100)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['complaint'] = df['complaint'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features and labels from the sampling for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['complaint'].values.tolist() \n",
    "y = df['label'].values.tolist()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer Trainer expects the data to be wrapped in an object with a predefined interface. For example a predefined method to get a data item. The following class is defined for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, by default the trainer outputs simple metrics like loss. It is possible to override by defining a custom metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_custom_metrics(p):\n",
    "    pred, labels = p\n",
    "    if(type(pred) is tuple):\n",
    "        pred = pred[0]\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred,average = 'macro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred,average = 'macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred,average = 'macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method trains a model using a given model and tokenizer. Data is tokenized and wrapped in the custom data wrapper. \n",
    "Then training arguments are configured. For example the evaluation_strategy set to \"steps\" (evaluate every eval_steps - also given). Alternatively set to \"epoch\" to evaluate at the end of each epoch.\n",
    "The trainer is given the wrapped data, training arguments, and the custom metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, X_train, X_val, y_train, y_val):\n",
    "    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=128)\n",
    "    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=128)    \n",
    "    train_dataset = Custom_Dataset(X_train_tokenized, y_train)\n",
    "    val_dataset = Custom_Dataset(X_val_tokenized, y_val)    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"output\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=10,\n",
    "        seed=0,\n",
    "        do_train=True,\n",
    "        do_eval=False,\n",
    "        warmup_steps=50,                \n",
    "        weight_decay=0.01,\n",
    "        logging_strategy='steps',        \n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\", \n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_custom_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the model metrics, confusion matrix and sklearn classification report will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix using code borrowed from scikit-learn.org\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_metrics(y_pred, y_val):\n",
    "    cm=confusion_matrix(y_val,y_pred, labels= [0,1,2,3,4])\n",
    "    print(cm)\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_val,y_pred, labels=[0,1,2,3,4]))\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plot_confusion_matrix(cm, classes=label_values, title='Confusion Matrix - Test Dataset')\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plot_confusion_matrix(cm, classes=label_values, title='Confusion Matrix - Test Dataset', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method compares a given list of models. It obtains a pre-trained tokenizer for the model. \n",
    "Then obtains the pre-trained model for classification. In Zero-shot classification a pre-trained classification model\n",
    "\"facebook/bart-large-mnli\" was used. Pre-trained classifiers have a classification head with the number of labels used during the pre-trained model training. In this experiment, the expectation is to leverage the language model only and then to generate a classification head\n",
    "specific to the supervised data. Therefore a base pre-trained language model should be used without a classification head.\n",
    "if a classification model like \"facebook/bart-large-mnli\" is used then the classification head should be removed, so that the \n",
    "model can learn a new classification head for the given labelled data.\n",
    "After training the model, predict the test data and select the most likely label. \n",
    "Then compute the classification performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models):\n",
    "    for model_name in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)    \n",
    "        trained_model = train_model(model, tokenizer, X_train, X_val, y_train, y_val)\n",
    "        raw_pred, _, _ = trained_model.predict(val_dataset)\n",
    "        if(type(raw_pred) is tuple):\n",
    "            raw_pred = raw_pred[0]\n",
    "        # Selected the most likely label\n",
    "        y_pred = np.argmax(raw_pred, axis=1)\n",
    "        compare_model_metrics(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 400\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 51/500 08:09 < 1:14:47, 0.10 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/13 00:13 < 00:19, 0.36 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"facebook/bart-base\", \"bert-base-uncased\"]\n",
    "compare_models(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n",
    "It is quite difficult to conduct a fair comparison of BART pre-training objectives against the previously employed pre-training objectives.There are differences in training data and resources, model architectural differences for example.\n",
    "Two evaluations were performed. Note that different models report different metrics: F1, EM, accuracy, Matthews correlation coefficient (MCC), mismatched version of MNLI (mnli-mm),  matched version of MNLI (mnli-m), Perplexities (PPL).\n",
    "\n",
    "## 5.1 Evaluation metrics reported in the paper\n",
    "### 5.1.1 Base model evaluation\n",
    "BART  implementations were compared with published numbers from BERT on similar training. BART was trained on 1M steps of books and Wikipedia data.All the following models are of base size.  There are 6 encoder and 6 decoder layers, with a hidden size of 768. That is 768 sized vector per word in the input sentence.\n",
    "\n",
    "Different BART pre-training methods were evaluated\n",
    "- Token Masking\n",
    "- Token Deletion\n",
    "- Text Infilling \n",
    "- Document Rotation \n",
    "- Sentence Shuffling\n",
    "- Text Infilling + Sentence Shuffling \n",
    "\n",
    "The following are the list of models compared against BART\n",
    "\n",
    "- BERT base \n",
    "- Language Model Similarly to GPT\n",
    "- Permuted language Model Based on XLNet\n",
    "- Masked language Model Following BERT\n",
    "- Multitask Masked language Model As in UniLM\n",
    "- Masked Seq-to-Seq inspired by MASS\n",
    "\n",
    "The following is the result of the comparison of multiple data sets.\n",
    "\n",
    "Table 1: Base model evaulation\n",
    "\n",
    "![Table 1](images/table1.png)\n",
    "Performance varies across tasks, however BART models with text infilling demonstrate consistently strong performance.\n",
    "\n",
    "\n",
    "### 5.1.2 Large scale evaluation\n",
    "BART was trained on the same scale as the RoBERTa model.Therefore a  large model with 12 encoder layers and 12 decoder layers with a hidden size of 1024. The training was performed on a batch size of 8000, and train the model for 500000 steps, similar to RoBERTa.\n",
    "The following is the evaluatation of BART against the following models for discriminative tasks\n",
    "\n",
    "Table 2: Large model evaluation for discriminative tasks.\n",
    "\n",
    "![Table 2](images/table2.png)\n",
    "BART performance is comparable to that of RoBERTa and XLNet. Therefore, can derive that BART’s uni-directional decoder layers do not reduce performance on discriminative tasks.\n",
    "\n",
    "The following is the evaluation of BART against the following models for summarization tasks.\n",
    "\n",
    "Table 3: Large model evaluation for summarization tasks with ROUGE scores.\n",
    "\n",
    "![Table 3](images/table3.png)\n",
    "As expected, BART outperforms previous work on summarization.\n",
    "\n",
    "## 5.1 Evaluation on a custom dataset for classification\n",
    "\n",
    "In section 4.1.2.2 an experiment was carried out on a classification task on a custom dataset. The purpose of the classification was to identify the most likely product category label given a complaint narrative.\n",
    "\n",
    "The goal was to compare the classification performance of BART and BERT.\n",
    "BART should consistently perform all downstream tasks. BERT is supposed to perform better on discriminative tasks than BART.\n",
    "\n",
    "The following are the metrics for the BERT base and BART base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion and Future Direction\n",
    "\n",
    "BART architecture is not that much different from the standard transformer architecture. The main experimentation was around pre-training. \n",
    "The paper's experiment on pretraining tasks may be concluded as follows: \n",
    "\n",
    "- The effectiveness of pretraining approaches varies greatly depending on the task. However, the BART models with text infilling achieved consistently stronger performance.\n",
    "- BART consistently delivers strong results. Therefore, suitable as a generic model.\n",
    "- The importance of token masking cannot be overstated. \n",
    "- Pretraining from left to right enhances generation. \n",
    "- Architectural considerations are also important other than the pre-training tasks. For example, relative position embeddings and segment-level recurrence impact the performance\n",
    "\n",
    "\n",
    "The strength of BART is also its weakness. The pretraining experimentation was to generate a generic language model that could be used across multiple downstream fine-tuning end tasks. Future work should then explore new methods of corrupting documents, maybe with the intent of being specific for certain downstream fine-tuning end tasks than being generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv, abs/1810.04805.\n",
    "\n",
    "[2]:  Liu, Yinhan et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” ArXiv abs/1907.11692 (2019): n. pag.\n",
    "\n",
    "[3]:  Yang, Zhilin et al. “XLNet: Generalized Autoregressive Pretraining for Language Understanding.” NeurIPS (2019).\n",
    "\n",
    "[4]: Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2020). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. ArXiv, abs/1910.13461.\n",
    "\n",
    "[5]:  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–\n",
    "4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\n",
    "\n",
    "[6]:  L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H. Hon (2019) Unified language model pre-training for natural language understanding and generation. arXiv preprint arXiv:1905.03197. \n",
    "\n",
    "[7]:  Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openaiassets/researchcovers/languageunsupervised/languageunderstanding paper. pdf, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
